{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":100366,"databundleVersionId":12053469,"sourceType":"competition"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/dataaddict71/material-informatics-ensemble?scriptVersionId=238541993\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport gc\nimport numpy as np\nimport pandas as pd\nimport random\nimport warnings\nimport sys\nimport subprocess\nimport time\nfrom tqdm.notebook import tqdm\n\n# --- Install Required Libraries ---\ndef install_package(package_name):\n    \"\"\"Checks and installs package via pip if missing.\"\"\"\n    try: __import__(package_name.split('-')[0].split('.')[0])\n    except ImportError:\n        print(f\"Package '{package_name}' not found. Attempting installation...\")\n        try:\n            process = subprocess.run( [sys.executable, \"-m\", \"pip\", \"install\", package_name, \"--no-cache\"], capture_output=True, text=True, check=True, timeout=300 )\n            print(f\"pip install {package_name} successful.\")\n            try: __import__(package_name.split('-')[0].split('.')[0]); print(f\"{package_name} imported.\")\n            except ImportError: print(f\"ERROR: Failed import {package_name} after install.\")\n        except Exception as e: print(f\"ERROR during {package_name} install: {e}\")\n\n# --- Setup Basic Output ---\n\nrequired_packages = [\"lightgbm\", \"tqdm\", \"scikit-learn\", \"pandas\", \"numpy\"]\nprint(f\"Checking/installing required packages: {required_packages}\")\nfor pkg in required_packages: install_package(pkg)\nprint(\"Package check/installation complete.\")\n\n# --- Standard Imports ---\nfrom sklearn.model_selection import LeaveOneOut, RandomizedSearchCV, KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import Ridge\nfrom sklearn.svm import SVR\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import RBF, Matern, WhiteKernel, ConstantKernel as C, ExpSineSquared, RationalQuadratic\nimport lightgbm as lgb\nfrom scipy.stats import uniform, loguniform\n\nwarnings.filterwarnings(\"ignore\")\ntqdm.pandas()\n\n# --- Configuration ---\nCONFIG = {\n    \"seed\": 42,\n    \"n_loocv_splits\": None, \n    \"n_tuning_cv_folds\": 5,\n    \"n_tuning_trials\": 30,\n    \"models_to_tune\": [\"svr_rbf\", \"gp\", \"lgbm\"],\n    \"models\": {\n        \"ridge\": {\"alpha\": 1.0, \"solver\": \"auto\"},\n        \"svr_rbf\": {\"kernel\": \"rbf\", \"C\": 1.0, \"gamma\": \"scale\", \"epsilon\": 0.1},\n        \"gp\": { \"kernel\": C(1.0) * RBF(1.0) + WhiteKernel(0.1), \"n_restarts_optimizer\": 15, \"normalize_y\": True, \"random_state\": 42, },\n        \"lgbm\": { \"objective\": \"regression_l2\", \"metric\": \"rmse\", \"n_estimators\": 100, \"learning_rate\": 0.05, \"num_leaves\": 5, \"max_depth\": 3, \"feature_fraction\": 0.8, \"bagging_fraction\": 0.8,\"bagging_freq\": 1, \"lambda_l1\": 0.2, \"lambda_l2\": 0.2, \"min_child_samples\": 5, \"verbose\": -1, \"n_jobs\": -1, \"seed\": 42, \"boosting_type\": \"gbdt\", },\n    },\n    \"output_dir\": \"./\",\n    \"ensemble_weights\": \"rmse\",\n}\nPARAM_GRIDS = {\n    \"svr_rbf\": { 'C': loguniform(1e-1, 1e3), 'gamma': loguniform(1e-4, 1e1), 'epsilon': uniform(0.01, 0.5) },\n    \"gp\": { 'alpha': loguniform(1e-10, 1e-1) }, \n    \"lgbm\": { 'n_estimators': [50, 100, 150, 200], 'learning_rate': loguniform(0.01, 0.2), 'num_leaves': [3, 5, 7, 10], 'max_depth': [2, 3, 4], 'min_child_samples': [3, 5, 7], 'reg_alpha': uniform(0, 1), 'reg_lambda': uniform(0, 1), 'feature_fraction': uniform(0.6, 0.4), 'bagging_fraction': uniform(0.6, 0.4), }\n}\n# --- Seed Everything ---\ndef seed_everything(seed):\n    random.seed(seed); np.random.seed(seed); os.environ['PYTHONHASHSEED'] = str(seed)\n    try: import torch; torch.manual_seed(seed); \n    except ImportError: pass\n    print(f\"Seed set globally to {seed}\")\nseed_everything(CONFIG[\"seed\"])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Load Data & Prepare ---\nprint(\"--- Starting Data Loading and Processing ---\")\ntrain_df = None\ntest_df = None\nsample_sub = None\ncomposition_col = 'Composition (X)'\nproperty_col = 'Property (Y)'\ntry:\n    train_path = '/kaggle/input/spring-2025-regression-challenge/Training_Dataset.csv'\n    test_path = '/kaggle/input/spring-2025-regression-challenge/Test_Dataset.csv'\n    sample_sub_path = '/kaggle/input/spring-2025-regression-challenge/Sample_submission.csv'\n    if not os.path.exists(train_path): raise FileNotFoundError(train_path)\n    if not os.path.exists(test_path): raise FileNotFoundError(test_path)\n    if not os.path.exists(sample_sub_path): raise FileNotFoundError(sample_sub_path)\n\n    train_df = pd.read_csv(train_path); test_df = pd.read_csv(test_path); sample_sub = pd.read_csv(sample_sub_path)\n    if composition_col not in train_df.columns or property_col not in train_df.columns: raise ValueError(\"Missing req train cols\")\n    if composition_col not in test_df.columns: raise ValueError(\"Missing req test col\")\n    CONFIG[\"n_loocv_splits\"] = len(train_df)\n    print(f\"Train={train_df.shape}, Test={test_df.shape}, LOOCV Splits={CONFIG['n_loocv_splits']}\")\n\n    print(f\"Processing '{composition_col}' as numeric...\")\n    # Keep original test compositions safe\n    test_compositions_original = test_df[composition_col].copy()\n\n    train_df['X_numeric'] = pd.to_numeric(train_df[composition_col], errors='coerce')\n    test_df['X_numeric'] = pd.to_numeric(test_df[composition_col], errors='coerce')\n    train_nans = train_df['X_numeric'].isnull().sum(); test_nans = test_df['X_numeric'].isnull().sum()\n    train_mean = train_df['X_numeric'].mean(); train_mean = 0 if pd.isna(train_mean) else train_mean\n    if train_nans > 0: print(f\"Imputing {train_nans} train NaNs.\"); train_df['X_numeric'] = train_df['X_numeric'].fillna(train_mean)\n    if test_nans > 0: print(f\"Imputing {test_nans} test NaNs.\"); test_df['X_numeric'] = test_df['X_numeric'].fillna(train_mean) \n    if not pd.api.types.is_numeric_dtype(train_df['X_numeric']): raise TypeError(f\"'{composition_col}' not numeric.\")\n\n    X_train_np = train_df[['X_numeric']].values\n    X_test_np = test_df[['X_numeric']].values\n    y_train = train_df[property_col].values\n    print(f\"Data shapes: X_train={X_train_np.shape}, y_train={y_train.shape}, X_test={X_test_np.shape}\")\n    if X_train_np.shape[1] == 0: raise ValueError(\"X_train has zero features.\")\n\nexcept Exception as e: print(f\"Error loading/processing data: {e}\"); raise\n# --- Hyperparameter Tuning (Initial - Outside LOOCV\nprint(\"\\n--- Starting Initial Hyperparameter Tuning ---\")\nscaler_tune = StandardScaler(); X_train_scaled_tune = scaler_tune.fit_transform(X_train_np)\ntuned_params = {}\nfor model_name in CONFIG[\"models_to_tune\"]:\n    if model_name not in CONFIG[\"models\"] or model_name not in PARAM_GRIDS: continue\n    print(f\"--- Tuning Model: {model_name} ---\")\n    start_tune_time = time.time(); model_instance = None; param_dist = PARAM_GRIDS[model_name]\n    try:\n        if model_name == \"svr_rbf\": model_instance = SVR(kernel='rbf')\n        elif model_name == \"gp\": model_instance = GaussianProcessRegressor(**CONFIG[\"models\"][\"gp\"])\n        elif model_name == \"lgbm\": model_instance = lgb.LGBMRegressor(**CONFIG[\"models\"][\"lgbm\"])\n        else: print(f\"Tuning not impl for {model_name}\"); continue\n        tuning_cv = KFold(n_splits=CONFIG[\"n_tuning_cv_folds\"], shuffle=True, random_state=CONFIG[\"seed\"])\n        random_search = RandomizedSearchCV( estimator=model_instance, param_distributions=param_dist, n_iter=CONFIG[\"n_tuning_trials\"], scoring='neg_root_mean_squared_error', n_jobs=-1, cv=tuning_cv, random_state=CONFIG[\"seed\"], verbose=0 )\n        random_search.fit(X_train_scaled_tune, y_train)\n        best_params = random_search.best_params_; best_score = -random_search.best_score_\n        tuned_params[model_name] = best_params; CONFIG[\"models\"][model_name].update(best_params)\n        print(f\"Tuning Complete [{model_name}] ({time.time() - start_tune_time:.2f}s). Best Params: {best_params}. Best RMSE: {best_score:.6f}\")\n    except Exception as e: print(f\"Error tuning {model_name}: {e}\")\nprint(\"--- Initial Hyperparameter Tuning Finished ---\"); print(f\"Final model configurations: {CONFIG['models']}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Modeling with LOOCV ---\nprint(\"\\n--- Starting Modeling with Leave-One-Out Cross-Validation ---\")\nos.makedirs(CONFIG[\"output_dir\"], exist_ok=True)\nmodel_oof_predictions = {}; model_test_predictions = {}; model_oof_rmses = {}\nscaler = StandardScaler(); loo = LeaveOneOut()\n\nfor model_name, model_params in CONFIG[\"models\"].items():\n    print(f\"\\n===== Training Model: {model_name} =====\"); print(f\"Using Params: {model_params}\")\n    oof_preds = np.zeros_like(y_train, dtype=float)\n    test_preds_loo_folds = np.zeros((CONFIG[\"n_loocv_splits\"], len(test_df)), dtype=float)\n    fold_errors = 0; model = None\n\n    for i, (train_index, val_index) in enumerate(tqdm(loo.split(X_train_np), total=CONFIG[\"n_loocv_splits\"], desc=f\"{model_name} LOOCV\")):\n        try:\n            X_train_loo, X_val_loo = X_train_np[train_index], X_train_np[val_index]\n            y_train_loo, y_val_loo = y_train[train_index], y_train[val_index]\n            if X_train_loo.shape[1] == 0: print(f\"Fold {i}: 0 features!\"); fold_errors += 1; continue\n\n            scaler.fit(X_train_loo); X_train_loo_scaled = scaler.transform(X_train_loo); X_val_loo_scaled = scaler.transform(X_val_loo)\n            X_test_scaled = scaler.transform(X_test_np)\n\n            current_seed = CONFIG[\"seed\"] + i; model_params_fold = model_params.copy()\n            if 'random_state' in model_params_fold: model_params_fold['random_state'] = current_seed\n            if 'seed' in model_params_fold: model_params_fold['seed'] = current_seed\n\n            if model_name == \"ridge\": model = Ridge(**model_params_fold)\n            elif model_name == \"svr_rbf\": model = SVR(**model_params_fold)\n            elif model_name == \"gp\": model = GaussianProcessRegressor(**model_params_fold)\n            elif model_name == \"lgbm\": model = lgb.LGBMRegressor(**model_params_fold)\n            else: print(f\"Model '{model_name}' unknown. Skip.\"); fold_errors+=CONFIG[\"n_loocv_splits\"]; break\n\n            model.fit(X_train_loo_scaled, y_train_loo)\n            oof_pred = model.predict(X_val_loo_scaled)[0]\n            if not np.isfinite(oof_pred): print(f\"WARN: {model_name} split {i} OOF NaN/Inf\"); oof_pred = np.mean(y_train_loo)\n            oof_preds[val_index[0]] = oof_pred\n            test_pred = model.predict(X_test_scaled)\n            if not np.all(np.isfinite(test_pred)): print(f\"WARN: {model_name} split {i} test NaN/Inf\"); test_pred = np.nan_to_num(test_pred, nan=np.mean(y_train_loo))\n            test_preds_loo_folds[i, :] = test_pred\n\n        except Exception as e:\n            print(f\"ERROR model {model_name} split {i}: {e}\")\n            fold_errors += 1; split_train_mean = np.mean(y_train_loo) if len(y_train_loo) > 0 else np.mean(y_train)\n            oof_preds[val_index[0]] = split_train_mean; test_preds_loo_folds[i, :] = split_train_mean\n        finally:\n            if model is not None: del model\n\n    if fold_errors == CONFIG[\"n_loocv_splits\"]: print(f\"Model {model_name} failed ALL splits.\"); continue\n\n    model_oof_predictions[model_name] = oof_preds\n    model_test_predictions[model_name] = np.mean(test_preds_loo_folds, axis=0)\n    oof_rmse = np.sqrt(mean_squared_error(y_train, oof_preds))\n    model_oof_rmses[model_name] = oof_rmse\n    print(f\"Model: {model_name} | OOF RMSE: {oof_rmse:.6f} | Errors: {fold_errors}/{CONFIG['n_loocv_splits']}\")\n    del oof_preds, test_preds_loo_folds; gc.collect()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Ensemble Predictions ---\nprint(\"\\n--- Ensembling Model Predictions ---\")\nprint(f\"Models available: {list(model_test_predictions.keys())}\"); print(f\"OOF RMSEs: {model_oof_rmses}\")\nall_test_preds_dict = model_test_predictions; all_oof_preds_dict = model_oof_predictions\nfinal_test_predictions = None; final_oof_predictions_ensemble = None; ensemble_oof_rmse = np.inf\n\nif len(all_test_preds_dict) == 0:\n    print(\"ERROR: No models available! Submission with train mean.\")\n    final_test_predictions = np.full(len(test_df), y_train.mean())\n    final_oof_predictions_ensemble = np.full_like(y_train, y_train.mean(), dtype=float)\nelif len(all_test_preds_dict) == 1:\n    model_name = list(all_test_preds_dict.keys())[0]; print(f\"Only one model '{model_name}'. Using its predictions.\")\n    final_test_predictions = all_test_preds_dict[model_name]; final_oof_predictions_ensemble = all_oof_preds_dict[model_name]\n    ensemble_oof_rmse = model_oof_rmses[model_name]\nelse:\n    if CONFIG[\"ensemble_weights\"] == \"simple\":\n        print(f\"Using Simple Averaging ensemble for {len(all_test_preds_dict)} models.\")\n        final_test_predictions = np.mean(np.array(list(all_test_preds_dict.values())), axis=0)\n        final_oof_predictions_ensemble = np.mean(np.array(list(all_oof_preds_dict.values())), axis=0)\n    elif CONFIG[\"ensemble_weights\"] == \"rmse\":\n        print(f\"Using Inverse RMSE Weighted Averaging for {len(all_test_preds_dict)} models.\")\n        weights = []; valid_model_names = list(all_oof_preds_dict.keys())\n        for name in valid_model_names: rmse = model_oof_rmses.get(name, np.inf); weights.append(1.0 / (rmse + 1e-9))\n        weights = np.array(weights)\n        if np.sum(weights) == 0 or not np.all(np.isfinite(weights)): print(\"WARN: Invalid weights, using simple avg.\"); weights = np.ones(len(valid_model_names))\n        weights /= np.sum(weights); print(f\"Ensemble Weights ({valid_model_names}): {weights}\")\n        oof_preds_array = np.array([all_oof_preds_dict[name] for name in valid_model_names])\n        test_preds_array = np.array([all_test_preds_dict[name] for name in valid_model_names])\n        final_oof_predictions_ensemble = np.sum(oof_preds_array * weights[:, np.newaxis], axis=0)\n        final_test_predictions = np.sum(test_preds_array * weights[:, np.newaxis], axis=0)\n    else:\n        print(f\"WARN: Unknown ensemble strategy. Using simple avg.\")\n        final_test_predictions = np.mean(np.array(list(all_test_preds_dict.values())), axis=0)\n        final_oof_predictions_ensemble = np.mean(np.array(list(all_oof_preds_dict.values())), axis=0)\n\n    \n    if final_oof_predictions_ensemble is not None:\n        ensemble_oof_rmse = np.sqrt(mean_squared_error(y_train, final_oof_predictions_ensemble))\n        print(f\"Ensemble OOF RMSE ({CONFIG['ensemble_weights']} method): {ensemble_oof_rmse:.6f}\")\n\nprint(f\"Final Test Predictions shape: {final_test_predictions.shape}, sample: {final_test_predictions[:5]}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Create Submission File --\n\nprint(\"--- Creating Submission File ---\")\ntry:\n    \n    print(\"Reloading original test data and forcing 'Composition (X)' as string for submission keys...\")\n    test_path = '/kaggle/input/spring-2025-regression-challenge/Test_Dataset.csv'\n    if not os.path.exists(test_path): raise FileNotFoundError(test_path)\n\n    \n    test_df_orig_for_sub = pd.read_csv(\n        test_path,\n        dtype={composition_col: str} \n    )\n    print(f\"Reloaded test_df_orig_for_sub. '{composition_col}' dtype: {test_df_orig_for_sub[composition_col].dtype}\")\n\n    if composition_col not in test_df_orig_for_sub.columns:\n        raise ValueError(f\"Original test data missing '{composition_col}' on reload!\")\n    if len(test_df_orig_for_sub) != len(final_test_predictions):\n         raise ValueError(f\"Length mismatch! Reloaded test ({len(test_df_orig_for_sub)}) vs predictions ({len(final_test_predictions)}).\")\n\n    print(\"Creating submission DataFrame using original string compositions...\")\n    submission_df = pd.DataFrame({\n        composition_col: test_df_orig_for_sub[composition_col], \n        property_col: final_test_predictions\n    })\n    # --- End Reload & String Forcing ---\n\n    if len(submission_df) != len(sample_sub): print(f\"WARN: Submission length ({len(submission_df)}) != sample ({len(sample_sub)}).\")\n    submission_path = os.path.join(CONFIG[\"output_dir\"], \"submission.csv\")\n    abs_submission_path = os.path.abspath(submission_path); print(f\"Attempting to save submission to: {abs_submission_path}\")\n    submission_df.to_csv(submission_path, index=False)\n    if os.path.exists(submission_path): print(f\"Submission file created: {submission_path}. Size: {os.path.getsize(submission_path)} bytes\"); print(\"\\nPreview:\"); print(submission_df.head())\n    else: print(f\"ERROR: Submission file NOT FOUND after saving!\")\nexcept Exception as e: print(f\"FATAL: Error creating/saving submission file: {e}\"); print(f\"\\nFinal Preds:\\n{final_test_predictions}\")\n# --- Save OOF Predictions --\n\nprint(\"--- Saving OOF Predictions ---\")\noof_save_df = train_df[[composition_col, property_col]].copy()\nfor model_name, preds in model_oof_predictions.items(): oof_save_df[f'oof_{model_name}'] = preds\nif final_oof_predictions_ensemble is not None and len(model_oof_predictions) > 1: oof_save_df['oof_ensemble'] = final_oof_predictions_ensemble\noof_path = os.path.join(CONFIG[\"output_dir\"], \"oof_predictions.csv\")\ntry: oof_save_df.to_csv(oof_path, index=False); print(f\"OOF predictions saved: {oof_path}\")\nexcept Exception as e: print(f\"Failed to save OOF predictions file: {e}\")\n\n\n# --- Save Test Predictions ---\n\nprint(\"--- Saving Test Predictions ---\")\ntest_pred_df = test_df[[composition_col]].copy() \nfor model_name, preds in model_test_predictions.items(): test_pred_df[f'pred_{model_name}'] = preds\nif final_test_predictions is not None and len(model_test_predictions) > 0 : test_pred_df['pred_final_ensemble'] = final_test_predictions\ntest_pred_path = os.path.join(CONFIG[\"output_dir\"], \"test_predictions.csv\")\ntry: test_pred_df.to_csv(test_pred_path, index=False); print(f\"Test predictions saved: {test_pred_path}\")\nexcept Exception as e: print(f\"Failed to save Test predictions file: {e}\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}